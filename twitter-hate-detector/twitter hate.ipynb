{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c4316d",
   "metadata": {},
   "source": [
    "# Identify hate speech in Twitter tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53249fe5",
   "metadata": {},
   "source": [
    "Problem Statement:\n",
    "    \n",
    "    Twitter is the biggest platform where anybody and everybody can have their views heard. Some of these voices spread hate and negativity. Twitter is wary of its platform being used as a medium  to spread hate. You are a data scientist at Twitter, and you will help Twitter in identifying the tweets with hate speech and removing them from the platform. You will use NLP techniques, perform specific cleanup for tweets data, and make a robust model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7392407a",
   "metadata": {},
   "source": [
    "Content:\n",
    "\n",
    "    id: Identifier Number\n",
    "\n",
    "    label: \n",
    "        0 - Not Hate\n",
    "        1 - Hate\n",
    "\n",
    "\n",
    "    tweet: text of the tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e813a33a",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e13dbc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368b8c54",
   "metadata": {},
   "source": [
    "# Load and Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec890dc9",
   "metadata": {},
   "source": [
    "Tasks:\n",
    "    \n",
    "    Load the tweets file using read_csv function from Pandas package. \n",
    "\n",
    "    Get the tweets into a list for easy text cleanup and manipulation.\n",
    "\n",
    "    To cleanup: \n",
    "\n",
    "    Normalize the casing.\n",
    "\n",
    "    Using regular expressions, remove user handles. These begin with '@’.\n",
    "\n",
    "    Using regular expressions, remove URLs.\n",
    "\n",
    "    Using TweetTokenizer from NLTK, tokenize the tweets into individual terms.\n",
    "\n",
    "    Remove stop words.\n",
    "\n",
    "    Remove redundant terms like ‘amp’, ‘rt’, etc.\n",
    "\n",
    "    Remove ‘#’ symbols from the tweet while retaining the term.\n",
    "\n",
    "    Extra cleanup by removing terms with a length of 1.\n",
    "\n",
    "    Check out the top terms in the tweets:\n",
    "\n",
    "    First, get all the tokenized terms into one large list.\n",
    "\n",
    "    Use the counter and find the 10 most common terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fde21682",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"TwitterHate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df44ccf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2afc9492",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns =(r\"@\\w+|http\\S+\")\n",
    "tweets = np.array(df[\"tweet\"])\n",
    "for i,tweet in enumerate(tweets):\n",
    "    ###Remove User/URL###\n",
    "    tweet=re.sub(patterns, \"\",tweet)\n",
    "    ###lowercase###\n",
    "    tweets[i]=tweet.lower()\n",
    "df[\"tweet\"] = pd.Series(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af4e1dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "tweets = np.array(df[\"tweet\"])\n",
    "labels = df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1fbebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,tweet in enumerate(tweets):\n",
    "    tweets[i]=tknzr.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "428927e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\") + [\"rt\",\n",
    "                   \"amp\",\n",
    "                   \"etc\",\"...\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1e5faab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tweet in enumerate(tweets):\n",
    "    tweets[i] = [tok for tok in tweet if tok not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8116142",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,tweet in enumerate(tweets):\n",
    "    for n,tok in enumerate(tweet):\n",
    "        if tok[0]==\"#\":\n",
    "            tweets[i][n]=tok[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fab129d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tweet in enumerate(tweets):\n",
    "    tweets[i] = [tok for tok in tweet if len(tok)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1ae284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_tweets = []\n",
    "for i,tweet in enumerate(tweets):\n",
    "    for tok in tweet:\n",
    "        flat_tweets.append(tok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc009585",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_tweets = np.array(flat_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83506fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(253478,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d686cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter = Counter(flat_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "641cf9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('love', 2748), ('day', 2276), ('happy', 1684), ('time', 1131), ('life', 1118), ('like', 1047), (\"i'm\", 1018), ('today', 1013), ('new', 994), ('thankful', 946)]\n"
     ]
    }
   ],
   "source": [
    "print(Counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f34fc9c",
   "metadata": {},
   "source": [
    "Data formatting for predictive modeling:\n",
    "\n",
    "    Join the tokens back to form strings. This will be required for the \n",
    "    vectorizers.\n",
    "\n",
    "    Assign x and y.\n",
    "\n",
    "    Perform train_test_split using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca16f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,tweet in enumerate(tweets):\n",
    "    tweets[i]=\" \".join(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f80050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tweets\n",
    "y=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7d6083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=.3,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca39079a",
   "metadata": {},
   "source": [
    "We’ll use TF-IDF values for the terms as a feature to get into a vector space model.\n",
    "\n",
    "    Perform TF-IDF  vectorization\n",
    "\n",
    "    Instantiate with a maximum of 5000 terms in your vocabulary.\n",
    "\n",
    "    Fit and apply on the train set.\n",
    "\n",
    "    Apply on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47a36ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac1cef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = tfidf.fit_transform(x_train)\n",
    "x_test = tfidf.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a76a2dc",
   "metadata": {},
   "source": [
    "Model building: Ordinary Logistic Regression\n",
    "\n",
    "    Instantiate Logistic Regression from sklearn with default parameters.\n",
    "\n",
    "    Fit into  the train data.\n",
    "\n",
    "    Make predictions for the train and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "060bb0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e282ed03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c064c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1713a6",
   "metadata": {},
   "source": [
    "Model evaluation: Accuracy, recall, and f_1 score.\n",
    "\n",
    "    Report the accuracy on the train set.\n",
    "\n",
    "    Report the recall on the train set: decent, high, or low.\n",
    "\n",
    "    Get the f1 score on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf4915ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          non-hate  hate\n",
      "non-hate      8880    25\n",
      "hate           468   216\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(confusion_matrix(y_test, preds),index=[\"non-hate\",\"hate\"],columns=[\"non-hate\",\"hate\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9f584d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.95\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy:{accuracy_score(y_test, preds):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4eac369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      8905\n",
      "           1       0.90      0.32      0.47       684\n",
      "\n",
      "    accuracy                           0.95      9589\n",
      "   macro avg       0.92      0.66      0.72      9589\n",
      "weighted avg       0.95      0.95      0.94      9589\n",
      "\n",
      "Recall is low due to class imbalance.\n",
      "The F-1 scores show the model is good at predicting non-hate tweets, but fails to predict hate tweets.\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,preds))\n",
    "print(\"Recall is low due to class imbalance.\\nThe F-1 scores show the model is good at predicting non-hate tweets, but fails to predict hate tweets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2255f4c",
   "metadata": {},
   "source": [
    "Looks like you need to adjust the class imbalance, as the model seems to focus on the 0s.\n",
    "\n",
    "    Adjust the appropriate class in the LogisticRegression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336561f1",
   "metadata": {},
   "source": [
    "Using oversampling to adjust for class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ac51dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = RandomOverSampler(sampling_strategy='minority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24088a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets.reshape(X.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f10e44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_over, y_over= oversample.fit_resample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7de5df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_over=X_over.reshape(X_over.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d00d250",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_over_train,x_over_test,y_over_train,y_over_test = train_test_split(X_over,y_over,test_size=.1,random_state=42)\n",
    "x_over_train = tfidf.fit_transform(x_over_train)\n",
    "x_over_test = tfidf.transform(x_over_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c69ba7",
   "metadata": {},
   "source": [
    "# Retrain with overfitted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1101bc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(x_over_train,y_over_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "953833ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_over = model.predict(x_over_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f6f2087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0     1\n",
      "0  2794   177\n",
      "1    49  2924\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(confusion_matrix(y_over_test,preds_over)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd96617b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96      2971\n",
      "           1       0.94      0.98      0.96      2973\n",
      "\n",
      "    accuracy                           0.96      5944\n",
      "   macro avg       0.96      0.96      0.96      5944\n",
      "weighted avg       0.96      0.96      0.96      5944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_over_test,preds_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5476eb6c",
   "metadata": {},
   "source": [
    "Model performs well on overfitted data with default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74390c2c",
   "metadata": {},
   "source": [
    "# Regularization and Hyperparameted tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c50e540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e968662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01c3ff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_over = tfidf.fit_transform(X_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0faf2a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(class_weight=\"balanced\")\n",
    "params = {\n",
    "    \"penalty\":[\"l2\",\"l1\",\"elasticnet\"],\n",
    "    'C': [0.001, 0.01, 0.1, 1,10, 100, 1000,10000]\n",
    "}\n",
    "grid = GridSearchCV(model,params, cv=4,verbose=3,scoring=\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f69f58b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 24 candidates, totalling 96 fits\n",
      "[CV 1/4] END ...............C=0.001, penalty=l2;, score=0.887 total time=   0.0s\n",
      "[CV 2/4] END ...............C=0.001, penalty=l2;, score=0.882 total time=   0.0s\n",
      "[CV 3/4] END ...............C=0.001, penalty=l2;, score=0.881 total time=   0.0s\n",
      "[CV 4/4] END ...............C=0.001, penalty=l2;, score=0.895 total time=   0.0s\n",
      "[CV 1/4] END .................C=0.001, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 2/4] END .................C=0.001, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 3/4] END .................C=0.001, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 4/4] END .................C=0.001, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 1/4] END .........C=0.001, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 2/4] END .........C=0.001, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 3/4] END .........C=0.001, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 4/4] END .........C=0.001, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 1/4] END ................C=0.01, penalty=l2;, score=0.909 total time=   0.0s\n",
      "[CV 2/4] END ................C=0.01, penalty=l2;, score=0.906 total time=   0.0s\n",
      "[CV 3/4] END ................C=0.01, penalty=l2;, score=0.904 total time=   0.0s\n",
      "[CV 4/4] END ................C=0.01, penalty=l2;, score=0.910 total time=   0.0s\n",
      "[CV 1/4] END ..................C=0.01, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 2/4] END ..................C=0.01, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 3/4] END ..................C=0.01, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 4/4] END ..................C=0.01, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 1/4] END ..........C=0.01, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 2/4] END ..........C=0.01, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 3/4] END ..........C=0.01, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 4/4] END ..........C=0.01, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 1/4] END .................C=0.1, penalty=l2;, score=0.939 total time=   0.1s\n",
      "[CV 2/4] END .................C=0.1, penalty=l2;, score=0.938 total time=   0.1s\n",
      "[CV 3/4] END .................C=0.1, penalty=l2;, score=0.933 total time=   0.1s\n",
      "[CV 4/4] END .................C=0.1, penalty=l2;, score=0.937 total time=   0.1s\n",
      "[CV 1/4] END ...................C=0.1, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 2/4] END ...................C=0.1, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 3/4] END ...................C=0.1, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 4/4] END ...................C=0.1, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 1/4] END ...........C=0.1, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 2/4] END ...........C=0.1, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 3/4] END ...........C=0.1, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 4/4] END ...........C=0.1, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 1/4] END ...................C=1, penalty=l2;, score=0.983 total time=   0.3s\n",
      "[CV 2/4] END ...................C=1, penalty=l2;, score=0.983 total time=   0.4s\n",
      "[CV 3/4] END ...................C=1, penalty=l2;, score=0.982 total time=   0.4s\n",
      "[CV 4/4] END ...................C=1, penalty=l2;, score=0.982 total time=   0.3s\n",
      "[CV 1/4] END .....................C=1, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 2/4] END .....................C=1, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 3/4] END .....................C=1, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 4/4] END .....................C=1, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 1/4] END .............C=1, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 2/4] END .............C=1, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 3/4] END .............C=1, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 4/4] END .............C=1, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 1/4] END ..................C=10, penalty=l2;, score=0.994 total time=   0.6s\n",
      "[CV 2/4] END ..................C=10, penalty=l2;, score=0.995 total time=   0.5s\n",
      "[CV 3/4] END ..................C=10, penalty=l2;, score=0.995 total time=   0.5s\n",
      "[CV 4/4] END ..................C=10, penalty=l2;, score=0.995 total time=   0.4s\n",
      "[CV 1/4] END ....................C=10, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 2/4] END ....................C=10, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 3/4] END ....................C=10, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 4/4] END ....................C=10, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 1/4] END ............C=10, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 2/4] END ............C=10, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 3/4] END ............C=10, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 4/4] END ............C=10, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 1/4] END .................C=100, penalty=l2;, score=0.998 total time=   0.5s\n",
      "[CV 2/4] END .................C=100, penalty=l2;, score=0.997 total time=   0.5s\n",
      "[CV 3/4] END .................C=100, penalty=l2;, score=0.998 total time=   0.4s\n",
      "[CV 4/4] END .................C=100, penalty=l2;, score=0.997 total time=   0.5s\n",
      "[CV 1/4] END ...................C=100, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 2/4] END ...................C=100, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 3/4] END ...................C=100, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 4/4] END ...................C=100, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 1/4] END ...........C=100, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 2/4] END ...........C=100, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 3/4] END ...........C=100, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 4/4] END ...........C=100, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 1/4] END ................C=1000, penalty=l2;, score=0.998 total time=   0.4s\n",
      "[CV 2/4] END ................C=1000, penalty=l2;, score=0.998 total time=   0.5s\n",
      "[CV 3/4] END ................C=1000, penalty=l2;, score=0.999 total time=   0.5s\n",
      "[CV 4/4] END ................C=1000, penalty=l2;, score=0.997 total time=   0.4s\n",
      "[CV 1/4] END ..................C=1000, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 2/4] END ..................C=1000, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 3/4] END ..................C=1000, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 4/4] END ..................C=1000, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 1/4] END ..........C=1000, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 2/4] END ..........C=1000, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 3/4] END ..........C=1000, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 4/4] END ..........C=1000, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 1/4] END ...............C=10000, penalty=l2;, score=0.999 total time=   0.5s\n",
      "[CV 2/4] END ...............C=10000, penalty=l2;, score=0.998 total time=   0.5s\n",
      "[CV 3/4] END ...............C=10000, penalty=l2;, score=0.999 total time=   0.5s\n",
      "[CV 4/4] END ...............C=10000, penalty=l2;, score=0.997 total time=   0.4s\n",
      "[CV 1/4] END .................C=10000, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 2/4] END .................C=10000, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 3/4] END .................C=10000, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 4/4] END .................C=10000, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 1/4] END .........C=10000, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 2/4] END .........C=10000, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 3/4] END .........C=10000, penalty=elasticnet;, score=nan total time=   0.0s\n",
      "[CV 4/4] END .........C=10000, penalty=elasticnet;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, estimator=LogisticRegression(class_weight='balanced'),\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000],\n",
       "                         'penalty': ['l2', 'l1', 'elasticnet']},\n",
       "             scoring='recall', verbose=3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_over,y_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d5262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "731643d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10000,\n",
       " 'class_weight': 'balanced',\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'auto',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'lbfgs',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.set_params(**grid.best_params_)\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "154ec3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    x_train,x_test,y_train,y_test = train_test_split(X_over,y_over,test_size=.1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c4199eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10000, class_weight='balanced')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8bf8593",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_grid=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b94fd1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9794751009421265\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,pred_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "531e3631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          non-hate  hate\n",
      "non-hate      2855   116\n",
      "hate             6  2967\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(confusion_matrix(y_test,pred_grid),index=[\"non-hate\",\"hate\"],columns=[\"non-hate\",\"hate\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2212423a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98      2971\n",
      "           1       0.96      1.00      0.98      2973\n",
      "\n",
      "    accuracy                           0.98      5944\n",
      "   macro avg       0.98      0.98      0.98      5944\n",
      "weighted avg       0.98      0.98      0.98      5944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,pred_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "97cb01df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10000, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ef9be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
